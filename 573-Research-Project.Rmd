---
title: "ECON573 Final Project"
author: 'Nicholas Wong'
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  pdf_document: default
---

R set-up 

```{r message = F}
library(tidyverse)
library(ISLR)
library(leaps)
library(glmnet)
library(pls)
library(MASS)
library(caret)
library(corrplot)
library(ggplot2)
library(sf)
library(RColorBrewer)
library(gridExtra)
library(modelr)
library(knitr)

airbnb <- read_csv("airbnb.csv",
                  col_select = -c("id", "amenities", "description", "name", "thumbnail_url", "neighbourhood", "zipcode"))
```

**Data cleaning/pre-processing** 
```{r}
set.seed(123)

# Mutate original price variable 
airbnb <- airbnb |> 
  mutate(price = exp(log_price)) 

# Drop NAs
airbnb <- na.omit(airbnb) 

# host_response_rate to numeric for easier interpretation 
airbnb <- airbnb |> 
  mutate(host_response_rate = str_replace_all(host_response_rate, pattern = "%", replacement = "")) |> 
  mutate_at(13, as.numeric) 

# 80/20 train/test split 
training_indices <- sample(1:nrow(airbnb), .8*nrow(airbnb))

# Split data into train and test sets 
train <- airbnb[training_indices, ]
test <- airbnb[-training_indices, ] # true unseen data for model testing

totalData <- rbind(train, test)
for (f in 1:length(names(totalData))) {
  levels(train[, f]) <- levels(totalData[, f])
  levels(test[,f]) <- levels(totalData[, f])
}
```

# Method 1 - Forward Selection # 

Here, we use a validation set approach due to the heavy computational expense of using stepwise methods with K-fold CV. 

_Fitting forward selection on training set_
```{r}
full = lm(price ~., data=train)
none = lm(price ~., data = train)
MSE = (summary(full)$sigma)^2
forward_selection_mod <- step(none, scope = list(upper = full), scale = MSE, direction = 'forward', trace=T)

summary(forward_selection_mod)
plot(forward_selection_mod)
```

Get predictions and residuals for forward selection, compute MSE 
```{r}
test_forwardselection <- test |> add_predictions(forward_selection_mod, var = "forward_pred")
test_forwardselection <- test_forwardselection |> add_residuals(forward_selection_mod, var = "forward_resid")

# Args: vector of residuals
# Return: RMSE 
RMSE_func <- function(resid){
  return(sqrt(mean(resid^2)))
}

RMSE_func(test_forwardselection$forward_resid)
```

# Method 2 - LASSO # 

Here, we select shrinkage parameter $\lambda$ for LASSO through repeated 5-fold CV. 
We test a range of 16 different $\lambda$ values in (0, 0.3), in equally spaced increments of 0.02.
```{r warning = F}
ctrl <- trainControl(method = "repeatedcv", number = 5, repeats = 10, verboseIter = F)

set.seed(1)

model_lasso <- train(price ~ ., 
                   data = train, 
                   method = "glmnet", 
                   preProcess = c("center", "scale"), 
                     metric = "RMSE",
                     maximize = F,
                     trControl = ctrl, 
                     tuneGrid = expand.grid(alpha = 1, # lasso specification
                                            lambda = seq(0, 0.3, 0.02)))

model_lasso$results |> 
  rename(CV_RMSE = RMSE) |> 
  mutate(min_CV_RMSE = as.numeric(lambda == model_lasso$bestTune$lambda)) |> 
  ggplot(aes(x = lambda, y = CV_RMSE)) + 
  geom_line(col = "grey55") + 
  geom_point(size = 2, aes(col = factor(min_CV_RMSE))) + 
  scale_color_manual(values = c("deepskyblue3", "green")) + 
  theme(legend.position = "none") + 
  labs(title = "AirBnB - Lasso Regression", 
       subtitle = "Selecting shrinkage parameter with cross-validation",
       y = "CV RMSE")
```


Optimal shrinkage ($\lambda$): 
```{r}
model_lasso$bestTune$lambda 
```

CV RMSE:
```{r eval = F}
(lasso_cv <- min(model_lasso$results$RMSE) |>  round(4))
```

Test RMSE: 
```{r eval = F}
(lasso_test <- sqrt(mean((predict(model_lasso, test) - test$price)^2)) |>  round(4))
```

Predictors in final fitted LASSO model: 
```{r}
tibble(names = model_lasso$coefnames) |> kable()
```

# Method 3 - Boosting # 



#########################################################################################################################


#########################################################################################################################

# Cluster Analysis # 

Top 3 priced listings
```{r eval = F}
head(airbnb |> arrange(desc(log_price)) |>  dplyr::select(log_price, city), 3) |> kable()
```
2 out of 3 top listings are from NYC. Perform cluster analysis in NYC 

_Beginning NYC cluster analysis_ 

This section requires the accompanying files nyc_boundaries.shp and nyc_boundaries.shx to run (to aid with the geographic map overlay).
```{r, eval = F}
airbnb <- read_csv("airbnb.csv",
                  col_select = -c("id", "amenities", "description", "name", "thumbnail_url"))

airbnb_NYC <- airbnb |> 
  filter(city == 'NYC') |> 
  filter(!is.na(neighbourhood)) |> 
  filter(log_price >= 0.01)
```

Here, we create a best_value variable for NYC neighbourhoods, and pick the top 500 to work with.
```{r, eval = F}
airbnb_NYC |> 
  group_by(neighbourhood) |> 
  filter(!is.na(neighbourhood)) |> 
  mutate(best_value = review_scores_rating/log_price) |> 
  arrange(desc(best_value))

top500_NYC <- head(airbnb_NYC, 500)
```

```{r, eval = F}
num_listings = table(airbnb_NYC$neighbourhood)

top500_NYC_nbr <- top500_NYC |> 
  group_by(neighbourhood) |> 
  filter(num_listings[as.character(neighbourhood)] > 50) |> 
  summarize(count = n()) |> 
  mutate(standardized_value = (round(100 * count/num_listings[as.character(neighbourhood)], digits = 4))) |>
  mutate(count_listings = num_listings[as.character(neighbourhood)]) |> 
  arrange(desc(standardized_value)) |> 
  ungroup()
```

```{r, eval = F}
top500_locations <- full_join(top500_NYC_nbr, top500_NYC, join_by(neighbourhood)) |> 
  dplyr::select(neighbourhood, count, standardized_value, longitude, latitude)
  
top500 <- top500_locations |> 
  group_by(neighbourhood) |> 
  mutate(mean_long = mean(longitude),
         mean_lat = mean(latitude),
         standardized_value = as.numeric(standardized_value)) |> 
  arrange(desc(standardized_value)) |> 
  distinct(neighbourhood, standardized_value, count, mean_long, mean_lat)
  
top10_nbr <- head(top500, 10)
```

Here, we calculate density of best value listings based on standardized best value in neighborhoods within the top 500 Best Value AirBnb listings in NY. 
```{r, eval = F}
nyc <- sf::st_read(dsn = "nyc_boundaries.shp", quiet = TRUE)

  ggplot() +
  geom_sf(data = nyc, fill = "#d9f2f2") +
  ggtitle("Best Value AirBnbs in NYC Neighborhoods") +
  geom_point(data = top10_nbr, aes(x = mean_long, y = mean_lat, size = as.numeric(standardized_value), color = as.factor(neighbourhood))) +
    scale_size_continuous(range = c(1, 4)) +
  guides(size = guide_legend(title = "Density of Best Value Listings")) +
  guides(color = guide_legend(title = "Neighborhood")) + theme_minimal()
```